{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Its-Itachi/Updated_PIFuHD/blob/main/Updated_PIFuHD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vZaAyhUJ9QC"
      },
      "source": [
        "## Requirements\n",
        "- Python 3\n",
        "- PyTorch tested on 1.4.0\n",
        "- json\n",
        "- PIL\n",
        "- skimage\n",
        "- tqdm\n",
        "- numpy\n",
        "- cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYhlsDkg1Hwb"
      },
      "source": [
        "## Clone PIFuHD repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmpEwdOd1G1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e68011f8-1e49-46e0-be18-b801a3db2f02"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/pifuhd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pifuhd'...\n",
            "remote: Enumerating objects: 222, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 222 (delta 20), reused 20 (delta 20), pack-reused 200 (from 1)\u001b[K\n",
            "Receiving objects: 100% (222/222), 399.83 KiB | 1.54 MiB/s, done.\n",
            "Resolving deltas: 100% (113/113), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ensure the apps/ folder exists\n",
        "os.makedirs(\"apps\", exist_ok=True)\n",
        "\n",
        "# Path to the recon.py inside the repo\n",
        "file_path = \"apps/recon.py\"\n",
        "\n",
        "# Full updated recon.py code with PyTorch 2.6+ safe loading\n",
        "patched_code = \"\"\"\n",
        "# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
        "ROOT_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import inv\n",
        "\n",
        "from lib.options import BaseOptions\n",
        "from lib.mesh_util import save_obj_mesh_with_color, reconstruction\n",
        "from lib.data import EvalWPoseDataset, EvalDataset\n",
        "from lib.model import HGPIFuNetwNML, HGPIFuMRNet\n",
        "from lib.geometry import index\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# Safe loading for PyTorch 2.6+\n",
        "import argparse\n",
        "import torch.serialization as serialization\n",
        "\n",
        "parser = BaseOptions()\n",
        "\n",
        "def gen_mesh(res, net, cuda, data, save_path, thresh=0.5, use_octree=True, components=False):\n",
        "    image_tensor_global = data['img_512'].to(device=cuda)\n",
        "    image_tensor = data['img'].to(device=cuda)\n",
        "    calib_tensor = data['calib'].to(device=cuda)\n",
        "\n",
        "    net.filter_global(image_tensor_global)\n",
        "    net.filter_local(image_tensor[:, None])\n",
        "\n",
        "    try:\n",
        "        if net.netG.netF is not None:\n",
        "            image_tensor_global = torch.cat([image_tensor_global, net.netG.nmlF], 0)\n",
        "        if net.netG.netB is not None:\n",
        "            image_tensor_global = torch.cat([image_tensor_global, net.netG.nmlB], 0)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    b_min = data['b_min']\n",
        "    b_max = data['b_max']\n",
        "\n",
        "    try:\n",
        "        save_img_path = save_path[:-4] + '.png'\n",
        "        save_img_list = []\n",
        "        for v in range(image_tensor_global.shape[0]):\n",
        "            save_img = (np.transpose(image_tensor_global[v].detach().cpu().numpy(), (1,2,0)) * 0.5 + 0.5)[:, :, ::-1] * 255.0\n",
        "            save_img_list.append(save_img)\n",
        "        save_img = np.concatenate(save_img_list, axis=1)\n",
        "        cv2.imwrite(save_img_path, save_img)\n",
        "\n",
        "        verts, faces, _, _ = reconstruction(net, cuda, calib_tensor, res, b_min, b_max, thresh, use_octree=use_octree, num_samples=50000)\n",
        "        verts_tensor = torch.from_numpy(verts.T).unsqueeze(0).to(device=cuda).float()\n",
        "\n",
        "        color = np.zeros(verts.shape)\n",
        "        interval = 50000\n",
        "        for i in range(len(color)//interval + 1):\n",
        "            left = i*interval\n",
        "            right = (i+1)*interval if i != len(color)//interval else -1\n",
        "            net.calc_normal(verts_tensor[:, None, :, left:right], calib_tensor[:, None], calib_tensor)\n",
        "            nml = net.nmls.detach().cpu().numpy()[0]*0.5 + 0.5\n",
        "            color[left:right] = nml.T\n",
        "\n",
        "        save_obj_mesh_with_color(save_path, verts, faces, color)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "def gen_mesh_imgColor(res, net, cuda, data, save_path, thresh=0.5, use_octree=True, components=False):\n",
        "    image_tensor_global = data['img_512'].to(device=cuda)\n",
        "    image_tensor = data['img'].to(device=cuda)\n",
        "    calib_tensor = data['calib'].to(device=cuda)\n",
        "\n",
        "    net.filter_global(image_tensor_global)\n",
        "    net.filter_local(image_tensor[:, None])\n",
        "\n",
        "    try:\n",
        "        if net.netG.netF is not None:\n",
        "            image_tensor_global = torch.cat([image_tensor_global, net.netG.nmlF], 0)\n",
        "        if net.netG.netB is not None:\n",
        "            image_tensor_global = torch.cat([image_tensor_global, net.netG.nmlB], 0)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    b_min = data['b_min']\n",
        "    b_max = data['b_max']\n",
        "\n",
        "    try:\n",
        "        save_img_path = save_path[:-4] + '.png'\n",
        "        save_img_list = []\n",
        "        for v in range(image_tensor_global.shape[0]):\n",
        "            save_img = (np.transpose(image_tensor_global[v].detach().cpu().numpy(), (1,2,0)) * 0.5 + 0.5)[:, :, ::-1] * 255.0\n",
        "            save_img_list.append(save_img)\n",
        "        save_img = np.concatenate(save_img_list, axis=1)\n",
        "        cv2.imwrite(save_img_path, save_img)\n",
        "\n",
        "        verts, faces, _, _ = reconstruction(net, cuda, calib_tensor, res, b_min, b_max, thresh, use_octree=use_octree, num_samples=100000)\n",
        "        verts_tensor = torch.from_numpy(verts.T).unsqueeze(0).to(device=cuda).float()\n",
        "\n",
        "        xyz_tensor = net.projection(verts_tensor, calib_tensor[:1])\n",
        "        uv = xyz_tensor[:, :2, :]\n",
        "        color = index(image_tensor[:1], uv).detach().cpu().numpy()[0].T\n",
        "        color = color * 0.5 + 0.5\n",
        "\n",
        "        if 'calib_world' in data:\n",
        "            calib_world = data['calib_world'].numpy()[0]\n",
        "            verts = np.matmul(np.concatenate([verts, np.ones_like(verts[:, :1])],1), inv(calib_world).T)[:, :3]\n",
        "\n",
        "        save_obj_mesh_with_color(save_path, verts, faces, color)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "def recon(opt, use_rect=False):\n",
        "    state_dict_path = None\n",
        "    if opt.load_netMR_checkpoint_path is not None:\n",
        "        state_dict_path = opt.load_netMR_checkpoint_path\n",
        "    elif opt.resume_epoch < 0:\n",
        "        state_dict_path = f\"{opt.checkpoints_path}/{opt.name}_train_latest\"\n",
        "        opt.resume_epoch = 0\n",
        "    else:\n",
        "        state_dict_path = f\"{opt.checkpoints_path}/{opt.name}_train_epoch_{opt.resume_epoch}\"\n",
        "\n",
        "    start_id = opt.start_id\n",
        "    end_id = opt.end_id\n",
        "    cuda = torch.device(f\"cuda:{opt.gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if state_dict_path is not None and os.path.exists(state_dict_path):\n",
        "        print(\"Resuming from\", state_dict_path)\n",
        "        try:\n",
        "            serialization.add_safe_globals([argparse.Namespace])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            state_dict = torch.load(state_dict_path, map_location=cuda, weights_only=False)\n",
        "        except TypeError:\n",
        "            state_dict = torch.load(state_dict_path, map_location=cuda)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed loading checkpoint {state_dict_path}: {e}\") from e\n",
        "\n",
        "        dataroot = opt.dataroot\n",
        "        resolution = opt.resolution\n",
        "        results_path = opt.results_path\n",
        "        loadSize = opt.loadSize\n",
        "\n",
        "        opt = state_dict['opt']\n",
        "        opt.dataroot = dataroot\n",
        "        opt.resolution = resolution\n",
        "        opt.results_path = results_path\n",
        "        opt.loadSize = loadSize\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Checkpoint not found: {state_dict_path}\")\n",
        "\n",
        "    test_dataset = EvalDataset(opt) if use_rect else EvalWPoseDataset(opt)\n",
        "    print(\"test data size:\", len(test_dataset))\n",
        "    projection_mode = test_dataset.projection_mode\n",
        "\n",
        "    opt_netG = state_dict['opt_netG']\n",
        "    netG = HGPIFuNetwNML(opt_netG, projection_mode).to(device=cuda)\n",
        "    netMR = HGPIFuMRNet(opt, netG, projection_mode).to(device=cuda)\n",
        "\n",
        "    netMR.load_state_dict(state_dict['model_state_dict'])\n",
        "\n",
        "    os.makedirs(opt.checkpoints_path, exist_ok=True)\n",
        "    os.makedirs(opt.results_path, exist_ok=True)\n",
        "    os.makedirs(f\"{opt.results_path}/{opt.name}/recon\", exist_ok=True)\n",
        "\n",
        "    if start_id < 0: start_id = 0\n",
        "    if end_id < 0: end_id = len(test_dataset)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        netG.eval()\n",
        "        print(\"generate mesh (test) ...\")\n",
        "        for i in tqdm(range(start_id, end_id)):\n",
        "            if i >= len(test_dataset):\n",
        "                break\n",
        "            test_data = test_dataset[i]\n",
        "            save_path = f\"{opt.results_path}/{opt.name}/recon/result_{test_data['name']}_{opt.resolution}.obj\"\n",
        "            print(save_path)\n",
        "            gen_mesh(opt.resolution, netMR, cuda, test_data, save_path, components=opt.use_compose)\n",
        "\n",
        "def reconWrapper(args=None, use_rect=False):\n",
        "    opt = parser.parse(args)\n",
        "    recon(opt, use_rect)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    reconWrapper()\n",
        "\"\"\"\n",
        "\n",
        "# Write the file\n",
        "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(patched_code)\n",
        "\n",
        "print(f\"✅ Custom recon.py written to {file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-luKVxp1DkBo",
        "outputId": "36ef19bf-4a9d-4924-b10d-21955fe7dcc3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Custom recon.py written to apps/recon.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQm-A8ESKb2"
      },
      "source": [
        "## Configure input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvle9T10fB6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f439f9ef-98b7-4319-e351-e8815d171cca"
      },
      "source": [
        "cd /content/pifuhd/sample_images"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pifuhd/sample_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SI7Ye1JfIim"
      },
      "source": [
        "**If you want to upload your own picture, run the next cell**. Otherwise, go to the next next cell. Currently PNG, JPEG files are supported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaV_7Yi8fM-B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "0b4a6139-9832-40cf-9cce-97118ecf7a6c"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "filename = list(files.upload().keys())[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8c6fe2e9-3fd4-4289-bffe-4da0e7596e5b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8c6fe2e9-3fd4-4289-bffe-4da0e7596e5b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2.jpeg to 2 (1).jpeg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEzmmB01SOZp"
      },
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "  image_path = '/content/pifuhd/sample_images/%s' % filename\n",
        "except:\n",
        "  image_path = '/content/pifuhd/sample_images/test.png' # example image\n",
        "image_dir = os.path.dirname(image_path)\n",
        "file_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "\n",
        "# output pathes\n",
        "obj_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.obj' % file_name\n",
        "out_img_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.png' % file_name\n",
        "video_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.mp4' % file_name\n",
        "video_display_path = '/content/pifuhd/results/pifuhd_final/result_%s_256_display.mp4' % file_name"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "896EC7iQfXkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5805980-a0ca-4a22-8b84-bc8c8f890247"
      },
      "source": [
        "cd /content"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbVmda9J5TDL"
      },
      "source": [
        "## Preprocess (for cropping image)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtMjWGNU5STe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ec9868-c904-4f31-a019-41d3c88fcbe9"
      },
      "source": [
        "!git clone https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'lightweight-human-pose-estimation.pytorch' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-vYklhI5dab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "786c2f7d-1b0f-4074-f552-94a75878bf40"
      },
      "source": [
        "cd /content/lightweight-human-pose-estimation.pytorch/"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lightweight-human-pose-estimation.pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRod9SOu77I6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374ef31b-7b33-4f30-ddf2-b0e4460298a2"
      },
      "source": [
        "!wget https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-10 17:19:21--  https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n",
            "Resolving download.01.org (download.01.org)... 23.42.200.215, 2600:1406:5400:2ac::a87, 2600:1406:5400:2a0::a87\n",
            "Connecting to download.01.org (download.01.org)|23.42.200.215|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87959810 (84M) [application/octet-stream]\n",
            "Saving to: ‘checkpoint_iter_370000.pth.1’\n",
            "\n",
            "checkpoint_iter_370 100%[===================>]  83.88M   104MB/s    in 0.8s    \n",
            "\n",
            "2025-10-10 17:19:22 (104 MB/s) - ‘checkpoint_iter_370000.pth.1’ saved [87959810/87959810]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdRcDXe38lHB"
      },
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
        "from modules.keypoints import extract_keypoints, group_keypoints\n",
        "from modules.load_state import load_state\n",
        "from modules.pose import Pose, track_poses\n",
        "import demo\n",
        "\n",
        "def get_rect(net, images, height_size):\n",
        "    net = net.eval()\n",
        "\n",
        "    stride = 8\n",
        "    upsample_ratio = 4\n",
        "    num_keypoints = Pose.num_kpts\n",
        "    previous_poses = []\n",
        "    delay = 33\n",
        "    for image in images:\n",
        "        rect_path = image.replace('.%s' % (image.split('.')[-1]), '_rect.txt')\n",
        "        img = cv2.imread(image, cv2.IMREAD_COLOR)\n",
        "        orig_img = img.copy()\n",
        "        orig_img = img.copy()\n",
        "        heatmaps, pafs, scale, pad = demo.infer_fast(net, img, height_size, stride, upsample_ratio, cpu=False)\n",
        "\n",
        "        total_keypoints_num = 0\n",
        "        all_keypoints_by_type = []\n",
        "        for kpt_idx in range(num_keypoints):  # 19th for bg\n",
        "            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n",
        "\n",
        "        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs)\n",
        "        for kpt_id in range(all_keypoints.shape[0]):\n",
        "            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
        "            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
        "        current_poses = []\n",
        "\n",
        "        rects = []\n",
        "        for n in range(len(pose_entries)):\n",
        "            if len(pose_entries[n]) == 0:\n",
        "                continue\n",
        "            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n",
        "            valid_keypoints = []\n",
        "            for kpt_id in range(num_keypoints):\n",
        "                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n",
        "                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
        "                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
        "                    valid_keypoints.append([pose_keypoints[kpt_id, 0], pose_keypoints[kpt_id, 1]])\n",
        "            valid_keypoints = np.array(valid_keypoints)\n",
        "\n",
        "            if pose_entries[n][10] != -1.0 or pose_entries[n][13] != -1.0:\n",
        "              pmin = valid_keypoints.min(0)\n",
        "              pmax = valid_keypoints.max(0)\n",
        "\n",
        "              center = (0.5 * (pmax[:2] + pmin[:2])).astype(np.int)\n",
        "              radius = int(0.65 * max(pmax[0]-pmin[0], pmax[1]-pmin[1]))\n",
        "            elif pose_entries[n][10] == -1.0 and pose_entries[n][13] == -1.0 and pose_entries[n][8] != -1.0 and pose_entries[n][11] != -1.0:\n",
        "              # if leg is missing, use pelvis to get cropping\n",
        "              center = (0.5 * (pose_keypoints[8] + pose_keypoints[11])).astype(np.int)\n",
        "              radius = int(1.45*np.sqrt(((center[None,:] - valid_keypoints)**2).sum(1)).max(0))\n",
        "              center[1] += int(0.05*radius)\n",
        "            else:\n",
        "              center = np.array([img.shape[1]//2,img.shape[0]//2])\n",
        "              radius = max(img.shape[1]//2,img.shape[0]//2)\n",
        "\n",
        "            x1 = center[0] - radius\n",
        "            y1 = center[1] - radius\n",
        "\n",
        "            rects.append([x1, y1, 2*radius, 2*radius])\n",
        "\n",
        "        np.savetxt(rect_path, np.array(rects), fmt='%d')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make checkpoints folder\n",
        "!mkdir -p checkpoints\n",
        "\n",
        "# Download the official PIFuHD checkpoint (for full-body reconstruction)\n",
        "!wget -O checkpoints/pifuhd.pt https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXKrnJFgRQVO",
        "outputId": "5c85d118-8c28-4237-9dd1-62b4230251f4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-10 17:19:30--  https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 99.84.41.33, 99.84.41.79, 99.84.41.129, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|99.84.41.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1548375177 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘checkpoints/pifuhd.pt’\n",
            "\n",
            "checkpoints/pifuhd. 100%[===================>]   1.44G   174MB/s    in 8.2s    \n",
            "\n",
            "2025-10-10 17:19:38 (181 MB/s) - ‘checkpoints/pifuhd.pt’ saved [1548375177/1548375177]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.int = int  # restore deprecated alias for compatibility\n"
      ],
      "metadata": {
        "id": "IbsDjqXIVOfx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6cGZD6f6IaY"
      },
      "source": [
        "net = PoseEstimationWithMobileNet()\n",
        "checkpoint = torch.load('checkpoint_iter_370000.pth', map_location='cpu')\n",
        "load_state(net, checkpoint)\n",
        "\n",
        "get_rect(net.cuda(), [image_path], 512)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0rgMInwTt0s"
      },
      "source": [
        "## Download the Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrIcZweSNRFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d8ceeb-2c67-4efc-def7-3d40facb4374"
      },
      "source": [
        "cd /content/pifuhd/"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pifuhd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3jjm6HuQRk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a191afed-d20e-4213-ffa4-6ae4ea43f33c"
      },
      "source": [
        "!sh ./scripts/download_trained_model.sh"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+ mkdir -p checkpoints\n",
            "+ cd checkpoints\n",
            "+ wget https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt pifuhd.pt\n",
            "--2025-10-10 17:19:47--  https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 99.84.41.33, 99.84.41.79, 99.84.41.129, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|99.84.41.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1548375177 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘pifuhd.pt.1’\n",
            "\n",
            "pifuhd.pt.1         100%[===================>]   1.44G   116MB/s    in 8.9s    \n",
            "\n",
            "2025-10-10 17:19:56 (166 MB/s) - ‘pifuhd.pt.1’ saved [1548375177/1548375177]\n",
            "\n",
            "--2025-10-10 17:19:56--  http://pifuhd.pt/\n",
            "Resolving pifuhd.pt (pifuhd.pt)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘pifuhd.pt’\n",
            "FINISHED --2025-10-10 17:19:56--\n",
            "Total wall clock time: 9.0s\n",
            "Downloaded: 1 files, 1.4G in 8.9s (166 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6heKcA-0QEBw"
      },
      "source": [
        "## Run PIFuHD!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls /content/pifuhd/apps\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMHJwXMMkXON",
        "outputId": "479e3ea4-0f43-4574-9947-5fcfd7e3d54d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_openpose.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  render_turntable.py\n",
            "clean_mesh.py      recon.py      simple_test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5995t2PnQTmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f188fcc8-a828-4f3b-a222-3c243f02591d"
      },
      "source": [
        "# Warning: all images with the corresponding rectangle files under -i will be processed.\n",
        "!python -m apps.simple_test -r 256 --use_rect -i $image_dir\n",
        "\n",
        "# seems that 256 is the maximum resolution that can fit into Google Colab.\n",
        "# If you want to reconstruct a higher-resolution mesh, please try with your own machine."
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from ./checkpoints/pifuhd.pt\n",
            "test data size: 2\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "generate mesh (test) ...\n",
            "  0% 0/2 [00:00<?, ?it/s]./results/pifuhd_final/recon/result_2 (1)_256.obj\n",
            "[ WARN:0@16.937] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
            " 50% 1/2 [00:07<00:07,  7.10s/it]./results/pifuhd_final/recon/result_2_256.obj\n",
            "100% 2/2 [00:11<00:00,  5.93s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3B8QdX2n4UUv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}